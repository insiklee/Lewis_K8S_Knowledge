# K8s Scheduler ê´€ë¦¬

# 01. ìŠ¤ì¼€ì¥´ëŸ¬ë€?

ë¦¬ì†ŒìŠ¤ë¥¼ ì–´ë–¤ ë…¸ë“œì— í• ë‹¹í•  ê²ƒì¸ì§€ ì •í•˜ëŠ” ì˜¤ë¸Œì íŠ¸ë‹¤.

# 02. ìŠ¤ì¼€ì¥´ë§ ë°©ì‹

ê¸°ë³¸ì ìœ¼ë¡œ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„°ëŠ” ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ í†µí•´ì„œ íŒŒë“œë¥¼ ì‹¤í–‰í•  ë…¸ë“œë¥¼ ì„ íƒí•œë‹¤. ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì€ ë‘ ê°€ì§€ë‹¤. ë°”ë¡œ **í•„í„°ë§**ê³¼ **ì ìˆ˜ ê³„ì‚°ë²•**ì´ë‹¤. 

**í•„í„°ë§** ë°©ì‹ì€ ì•„ë˜ì—ì„œ ì†Œê°œí•  nodeNameì´ë‚˜ nodeSelector, nodeAffinityë¥¼ í†µí•´ì„œ ì›í•˜ëŠ” ë…¸ë“œë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë‹¤.

**ì ìˆ˜ ê³„ì‚°ë²•**ì€ í•„í„°ë§ìœ¼ë¡œ ê±¸ëŸ¬ì§„ ë…¸ë“œë“¤ ì¤‘ì—ì„œ ìì› ì‚¬ìš©ëŸ‰ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•´ì„œ ë” ì ì ˆí•œ ë…¸ë“œì—ì„œ íŒŒë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë‹¤.

# 03. íŠ¹ì • ë…¸ë“œì—ì„œ ë¦¬ì†ŒìŠ¤ ì‹¤í–‰ ë°©ë²•

## 01. nodeName

nodeName ì˜µì…˜ìœ¼ë¡œ íŠ¹ì • íŒŒë“œì˜ ì‹¤í–‰ ë…¸ë“œë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤.

ìŠ¤ì¼€ì¥´ëŸ¬ì— ì˜í•´ ê´€ë¦¬ë˜ì§€ ì•Šê³  ì§€ì •ëœ ë…¸ë“œì˜ kubeletì´ Podë¥¼ ê´€ë¦¬í•œë‹¤. ë”°ë¼ì„œ drainì´ë‚˜ cordon ìƒíƒœë¥¼ ë¬´ì‹œí•˜ê³  podë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. 

**YAML ì˜ˆì‹œ**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nodename-pod
spec:
  nodeName: worker1.unnet.k8s.io
  containers:
  - image: nginx
    name: nginx
```

**YAML ì‹¤í–‰ ê²°ê³¼**

```json
**# kubectl apply -f nodeName-pod.yaml** 
pod/nodename-pod created

**# kubectl get pod -o wide**
NAME           READY   STATUS    RESTARTS   AGE   IP              NODE                   NOMINATED NODE   READINESS GATES
nodename-pod   1/1     Running   0          12s   172.16.79.152   worker1.unnet.k8s.io   <none>           <none>
```

**ì£¼ì˜) nodeNameì„ ì‚¬ìš©í•  ê²½ìš° cordon ë˜ì–´ì„œ Unscheduledëœ ë…¸ë“œì—ì„œë„ ê°•ì œë¡œ íŒŒë“œë¥¼ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆê²Œ ëœë‹¤.**

```json
**# kubectl get pod**
No resources found in default namespace.

**# kubectl cordon worker1.unnet.k8s.io** 
node/worker1.unnet.k8s.io cordoned

**# kubectl get nodes**
NAME                   STATUS                     ROLES           AGE   VERSION
master.unnet.k8s.io    Ready                      control-plane   20d   v1.25.0
**worker1.unnet.k8s.io   Ready,SchedulingDisabled**   <none>          20d   v1.25.0
worker2.unnet.k8s.io   Ready

**# kubectl apply -f nodeName-pod.yaml** 
pod/nodename-pod created

**# kubectl get pod**
NAME           READY   STATUS    RESTARTS   AGE
nodename-pod   1/1     **Running**   0          14s  <- unscheduledëœ ë…¸ë“œì—ì„œ ì‹¤í–‰ë¨
```

## 02. nodeSelector

nodeSelectorëŠ” ë…¸ë“œì— ë¶€ì—¬ëœ labelì„ ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

ë…¸ë“œì˜ hostnameì„ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒí•  ìˆ˜ ìˆê³ , nodeì— ë¶€ì—¬ëœ roleì„ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

### 01. íŠ¹ì • ë…¸ë“œ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°

kubernetes.io/hostname ë¼ë²¨ì„ ì‚¬ìš©í•´ì„œ íŠ¹ì • ë…¸ë“œë¥¼ ì„ íƒí•œë‹¤.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nodeselector-pod
spec:
  **nodeSelector:
    kubernetes.io/hostname: worker1.unnet.k8s.io**
  containers:
  - image: nginx
    name: container1
```

### 02. íŠ¹ì • ë¼ë²¨ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  **nodeSelector:
    disktype: ssd**
```

**YAML ì‹¤í–‰ ê²°ê³¼**

```json
**# kubectl apply -f nodeSelector-pod.yaml** 
pod/nodeselector-pod created

**# kubectl get pod -o wide**
NAME               READY   STATUS    RESTARTS   AGE   IP              NODE                   NOMINATED NODE   READINESS GATES
nodeselector-pod   1/1     Running   0          9s    172.16.79.154   **worker1.unnet.k8s.io**   <none>           <none>
```

cordonëœ ë…¸ë“œë¥¼ nodeSelectorë¡œ ì§€ì •í•˜ì—¬ Podë¥¼ ì‹¤í–‰í•  ê²½ìš° í•´ë‹¹ PodëŠ” Pendingì´ ëœë‹¤.

```json
**# kubectl get nodes**
NAME                   STATUS                     ROLES           AGE   VERSION
master.unnet.k8s.io    Ready                      control-plane   20d   v1.25.0
**worker1.unnet.k8s.io   Ready,SchedulingDisabled**   <none>          20d   v1.25.0
worker2.unnet.k8s.io   Ready                      <none>          20d   v1.25.0

**# kubectl apply -f nodeSelector-pod.yaml** 
pod/nodeselector-pod created

**# kubectl get pod**
NAME               READY   STATUS    RESTARTS   AGE
nodename-pod       1/1     Running   0          65m <- nodeNameìœ¼ë¡œ ì§€ì •í•˜ë©´ ì‹¤í–‰ëœë‹¤.
**nodeselector-pod   0/1     Pending   0          12s** <- nodeSelectorë¡œ ì„ íƒí•˜ë©´ íœë”©ëœë‹¤.
```

## 03. nodeAffinity

nodeAffinityëŠ” nodeSelector ë³´ë‹¤ ì¡°ê¸ˆ ë³µì¡í•œ êµ¬ë¬¸ì„ ìš”êµ¬í•˜ì§€ë§Œ, ê·¸ë§Œí¼ ë”ìš± ì„¸ë¶„í™”ëœ ì„¤ì •ì„ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤.

nodeAffinityëŠ” required node affinityì™€ preferred node affinityê°€ ìˆë‹¤.

### 01. Required Node Affinity

Required Node AffinityëŠ” Matchingëœ Labelì— ì†í•œ ë…¸ë“œì—ì„œë§Œ ì‹¤í–‰ì‹œí‚¤ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-affinity
  name: nginx-affinity
spec:
  **affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd**
  containers:
  - image: nginx
    name: nginx-affinity
  dnsPolicy: ClusterFirst
  restartPolicy: Always
```

Required Node AffinityëŠ” **requiredDuringSchedulingIgnoredDuringExecution:** êµ¬ë¬¸ì„ ì‚¬ìš©í•œë‹¤. ì™œì´ë ‡ê²Œ ê¸´ êµ¬ë¬¸ì„ ì‚¬ìš©í•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ ì™¸ìš°ê¸°ì—” ë„ˆë¬´ ê¸¸ê¸° ë•Œë¬¸ì— ê°€ê¸‰ì  ë³µì‚¬ë¥¼ í•˜ì.

ë¬´ì¡°ê±´ ë§¤ì¹­ëœ ë…¸ë“œì—ì„œë§Œ ì‹¤í–‰í•˜ê¸° ë•Œë¬¸ì— ê·¸ í•˜ìœ„ êµ¬ë¬¸ì€ nodeSelectorTermsê°€ ëœë‹¤.

### ë¼ë²¨ì´ í¬í•¨ëœ ë…¸ë“œê°€ ìˆëŠ” ê²½ìš°

nodeSelectorTemsì—ì„œ ë§¤ì¹­ë˜ëŠ” ë¼ë²¨ì´ ìˆëŠ” ë…¸ë“œì—ì„œ ì‹¤í–‰ëœë‹¤.

```json
# **kubectl get node -l disktype=ssd**
NAME          STATUS   ROLES    AGE     VERSION
k8s-worker1   Ready    <none>   3d21h   v1.24.1

**# kubectl apply -f nginx-affinity.yaml** 
pod/nginx-affinity created

**# kubectl get pod -o wide**
NAME                         READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES
nginx-affinity               1/1     Running   0          94s    10.100.194.120   k8s-worker1   <none>           <none>
```

### ë¼ë²¨ì´ í¬í•¨ëœ ë…¸ë“œê°€ ì—†ëŠ” ê²½ìš°

ë§¤ì¹­ë˜ëŠ” ë¼ë²¨ì´ ì—†ì„ ê²½ìš° Pending ìƒíƒœê°€ ë˜ë©° ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤.

```json
**# kubectl get node -l disktype=ssd**
No resources found

**# kubectl apply -f nginx-affinity.yaml** 
pod/nginx-affinity created

**# kubectl get pod -o wide**
NAME                         READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES
nginx-affinity               0/1     **Pending**   0          2s     <none>           <none>        <none>           <none>
```

### 02. Preferred Node Affinity

Preffered Node AffinityëŠ” Matchingëœ Labelì— ì†í•œ ë…¸ë“œì— ìš°ì„  ì‹¤í–‰ê¶Œì„ ì£¼ì§€ë§Œ, matching ë˜ëŠ” ë…¸ë“œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ë¥¸ nodeì—ì„œ ì‹¤í–‰í•˜ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-affinity
  name: nginx-affinity
spec:
  **affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd**
  containers:
  - image: nginx
    name: nginx-affinity
  dnsPolicy: ClusterFirst
  restartPolicy: Always
```

Perferred Node AffinityëŠ” **preferredDuringSchedulingIgnoredDuringExecution:** êµ¬ë¬¸ì„ ì‚¬ìš©í•œë‹¤. ì™œì´ë ‡ê²Œ ê¸´ êµ¬ë¬¸ì„ ì‚¬ìš©í•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ ì™¸ìš°ê¸°ì—” ë„ˆë¬´ ê¸¸ê¸° ë•Œë¬¸ì— ê°€ê¸‰ì  ë³µì‚¬ë¥¼ í•˜ì.

ë¬´ì¡°ê±´ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ìš°ì„ ìˆœìœ„ë¥¼ ë” ë¶€ì—¬í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— weight êµ¬ë¬¸ê³¼ prefference êµ¬ë¬¸ì„ ì‚¬ìš©í•œë‹¤.

**weight**ëŠ” 1ì—ì„œ 100ì‚¬ì´ì˜ ê°’ì„ ë„£ì„ ìˆ˜ ìˆë‹¤. preferenceì— ì¶©ì¡±ëœ ë…¸ë“œì—ê²Œ weight ë§Œí¼ì˜ ê°€ì‚°ì ì„ ë¶€ì—¬í•´ì„œ ì ìˆ˜ë¥¼ ë†’ì¸ë‹¤.

### ë¼ë²¨ì´ í¬í•¨ëœ ë…¸ë“œê°€ ìˆëŠ” ê²½ìš°

```json
# **kubectl get node -l disktype=ssd**
NAME          STATUS   ROLES    AGE     VERSION
k8s-worker1   Ready    <none>   3d21h   v1.24.1

**# kubectl apply -f nginx-affinity.yaml** 
pod/nginx-affinity created

**# kubectl get pod -o wide**
NAME                         READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES
nginx-affinity               1/1     Running   0          94s    10.100.194.120   k8s-worker1   <none>           <none>
```

### ë¼ë²¨ì´ í¬í•¨ëœ ë…¸ë“œê°€ ì—†ëŠ” ê²½ìš°

ì•„ë¬´ ë…¸ë“œë‚˜ ì‹¤í–‰ëœë‹¤.

```json
**# kubectl get node -l disktype=ssd**
No resources found

**# kubectl apply -f nginx-affinity.yaml** 
pod/nginx-affinity created

**# kubectl get pod -o wide**
NAME             READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
nginx-affinity   1/1     Running   0          7s    10.110.126.6   k8s-worker2   <none>           <none>
```

## 04. inter-pod Affinity / AntiAffinity

[Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#types-of-inter-pod-affinity-and-anti-affinity)

## 05. TopologySpreadConstraints

# 04. íŠ¹ì • ë…¸ë“œì˜ ìŠ¤ì¼€ì¥´ë§ ì œí•œ

## 01. Taint / toleration

TaintëŠ” ë”ëŸ½ë‹¤ëŠ” ëœ»ì´ë‹¤. ë”ëŸ¬ìš°ë‹ˆê¹Œ Podë¥¼ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ì—†ë‹¤. 

ë°˜ë©´ tolerationì€ ì—¬ê¸°ê°€ ë”ëŸ¬ì›Œë„ ë²„í‹¸ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ(tolerate) Podë¥¼ ì‹¤í–‰ì‹œí‚¤ê² ë‹¨ ëœ»ì´ë‹¤.

### 01. Taint ì„¤ì •

<aside>
ğŸ’¡ **# kubectl taint node {ë…¸ë“œëª…} {í‚¤}={ê°’}:NoSchedule**

</aside>

```json
**# kubectl get node**
NAME          STATUS   ROLES           AGE     VERSION
k8s-master    Ready    control-plane   3d22h   v1.24.1
k8s-worker1   Ready    <none>          3d22h   v1.24.1
k8s-worker2   Ready    <none>          3d22h   v1.24.1

**# kubectl taint node k8s-worker1 node-taint=taint-on:NoSchedule**
node/k8s-worker1 tainted

**# kubectl describe node k8s-worker1 | grep NoSchedule**
**Taints:             node-taint=taint-on:NoSchedule**

**## Taint ì ìš© í…ŒìŠ¤íŠ¸**
**# kubectl create deploy taint-test --image=nginx --replicas=5**
deployment.apps/taint-test created

# **kubectl get pod -o wide**
NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
taint-test-7dd4467485-288cn   1/1     Running   0          30s   10.110.126.12   **k8s-worker2**   <none>           <none>
taint-test-7dd4467485-5mnlv   1/1     Running   0          30s   10.110.126.15   **k8s-worker2**   <none>           <none>
taint-test-7dd4467485-bmqwj   1/1     Running   0          30s   10.110.126.19   **k8s-worker2**   <none>           <none>
taint-test-7dd4467485-fvl8g   1/1     Running   0          30s   10.110.126.17   **k8s-worker2**   <none>           <none>
taint-test-7dd4467485-llpzl   1/1     Running   0          30s   10.110.126.18   **k8s-worker2**   <none>           <none>
```

### 02. toleration ì„¤ì •

tolerationì€ yaml íŒŒì¼ì„ í†µí•´ ìˆ˜ì •í•´ì•¼ í•œë‹¤.

- **toleration êµ¬ë¬¸**
    - **í‚¤ : ê°’ ë™ì¼**
        
        operatorë¥¼ Equalë¡œ ì„¤ì •í•œ ë’¤, ì •í™•í•œ valueë¥¼ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤.
        
        ```yaml
        tolerations:
          - key: node-taint
            **operator: Equal
            value: taint-on**
            effect: NoSchedule
        ```
        
    - **í‚¤ì˜ ì¡´ì¬ ì—¬ë¶€**
        
        operatorë¥¼ Existsë¡œ ì„¤ì •í•˜ë©´, í•´ë‹¹ í‚¤ê°€ ì¡´ì¬ë§Œ í•´ë„ tolerationì´ ì ìš©ëœë‹¤.
        
        ```yaml
        tolerations:
          - key: node-taint
            **operator: Exists**
            effect: NoSchedule
        ```
        

**Yaml ì˜ˆì‹œ**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: taint-toleration-test
  name: taint-toleration-test
spec:
  replicas: 5
  selector:
    matchLabels:
      app: taint-toleration-test
  template:
    metadata:
      labels:
        app: taint-toleration-test
    spec:
      containers:
      - image: nginx
        name: nginx
      **tolerations:
      - key: node-taint
        operator: Equal
        value: taint-on
        effect: NoSchedule**
```

**ì ìš© ê²°ê³¼**

```json
**# kubectl apply -f taint-toleration-test.yaml** 
deployment.apps/taint-toleration-test created

**# kubectl get pod -o wide**
NOMINATED NODE   READINESS GATES
taint-toleration-test-688dd8b89-b6kcg   1/1     Running   0          21s   10.100.194.126   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-m9xlw   1/1     Running   0          21s   10.100.194.123   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-pkktn   1/1     Running   0          21s   10.110.126.24    **k8s-worker2**   <none>           <none>
taint-toleration-test-688dd8b89-xf5lx   1/1     Running   0          21s   10.100.194.122   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-z76f8   1/1     Running   0          21s   10.110.126.27    **k8s-worker2**   <none>           <none>
```

### 03. Taint í•´ì œ

<aside>
ğŸ’¡ **# kubectl taint node {nodeëª…} {í‚¤}-**

</aside>

```json
**# kubectl taint node k8s-worker1 node-taint-**
node/k8s-worker1 untainted

**# kubectl describe node k8s-worker1 | grep NoSchedule**
```

## 02. cordon / uncordon

cordonì€ ì €ì§€ì„ ì´ë¼ëŠ” ëœ»ì´ë‹¤. íŠ¹ì • ë…¸ë“œì— ì„ì‹œë¡œ Podê°€ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ì¼ì¢…ì˜ ì°¨ë‹¨ë²½ì„ ì„¤ì •í•œë‹¤. Nodeì˜ PMì´ë‚˜ Node ìƒì˜ OS ì‘ì—…, kubelet ì—…ê·¸ë ˆì´ë“œ ë“±ì˜ ì‘ì—…ì„ í•  ë•Œ ì‚¬ìš©í•œë‹¤.

### 01. cordon ì„¤ì •

<aside>
ğŸ’¡ **# kubectl cordon {ë…¸ë“œ ëª…}**

</aside>

```json
**# kubectl cordon k8s-worker1**
node/k8s-worker1 cordoned

# **kubectl get nodes**
NAME          STATUS                     ROLES           AGE     VERSION
k8s-master    Ready                      control-plane   3d22h   v1.24.1
k8s-worker1   Ready**,SchedulingDisabled**   <none>          3d22h   v1.24.1
k8s-worker2   Ready                      <none>          3d22h   v1.24.1
```

cordonì„ ì„¤ì •í•˜ë©´ í•´ë‹¹ ë…¸ë“œëŠ” ìë™ìœ¼ë¡œ Taint ì„¤ì •ì´ ì¶”ê°€ëœë‹¤. ì´ë•Œ í‚¤ëŠ” node.kubernetes.io/unschedulabeì´ ë˜ê³  ë”°ë¡œ ê°’ì€ ì§€ì •ë˜ì§€ ì•ŠëŠ”ë‹¤. 

```json
**# kubectl describe nodes k8s-worker1 | grep Taint**
Taints:             node.kubernetes.io/unschedulable:NoSchedule
**# kubectl get nodes k8s-worker1 -o jsonpath='{.spec.taints}' | jq .**
[
  {
    "effect": "NoSchedule",
    "key": "node.kubernetes.io/unschedulable",
    "timeAdded": "2022-09-30T08:15:36Z"
  }
]
```

ì£¼ì˜í•´ì•¼í•  ê²ƒì´ ìˆëŠ”ë° cordonì„ ì„¤ì •í•œë‹¤ í•˜ë”ë¼ë„ ê¸°ì¡´ì— ì‹¤í–‰ì¤‘ì¸ podë“¤ì´ ë‹¤ë¥¸ nodeë¡œ ì´ë™í•˜ì§€ ì•ŠëŠ”ë‹¤. 

```json
**## ì‘ì—… ì „ Pod ìƒíƒœ**
**# kubectl get pod -o wide**
NAME                                    READY   STATUS    RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
taint-toleration-test-688dd8b89-b6kcg   1/1     Running   0          8m34s   10.100.194.126   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-m9xlw   1/1     Running   0          8m34s   10.100.194.123   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-pkktn   1/1     Running   0          8m34s   10.110.126.24    k8s-worker2   <none>           <none>
taint-toleration-test-688dd8b89-xf5lx   1/1     Running   0          8m34s   10.100.194.122   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-z76f8   1/1     Running   0          8m34s   10.110.126.27    k8s-worker2   <none>           <none>

**## cordon ì‹¤í–‰
# kubectl cordon k8s-worker1**
node/k8s-worker1 cordoned

****# **kubectl get nodes**
NAME          STATUS                     ROLES           AGE     VERSION
k8s-master    Ready                      control-plane   3d22h   v1.24.1
k8s-worker1   Ready**,SchedulingDisabled**   <none>          3d22h   v1.24.1
k8s-worker2   Ready    

**## ì‘ì—… í›„ Pod ìƒíƒœ
# kubectl get pod -o wide**
NAME                                    READY   STATUS    RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
taint-toleration-test-688dd8b89-b6kcg   1/1     Running   0          8m34s   10.100.194.126   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-m9xlw   1/1     Running   0          8m34s   10.100.194.123   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-pkktn   1/1     Running   0          8m34s   10.110.126.24    k8s-worker2   <none>           <none>
taint-toleration-test-688dd8b89-xf5lx   1/1     Running   0          8m34s   10.100.194.122   **k8s-worker1**   <none>           <none>
taint-toleration-test-688dd8b89-z76f8   1/1     Running   0          8m34s   10.110.126.27    k8s-worker2   <none>           <none>
```

### 02. uncordon ì„¤ì •

<aside>
ğŸ’¡ **# kubectl uncordon {ë…¸ë“œëª…}**

</aside>

```json
# **kubectl get nodes**
NAME          STATUS                     ROLES           AGE     VERSION
k8s-master    Ready                      control-plane   3d22h   v1.24.1
k8s-worker1   Ready**,SchedulingDisabled**   <none>          3d22h   v1.24.1
k8s-worker2   Ready    

**# kubectl uncordon k8s-worker1**
node/k8s-worker1 uncordoned

# **kubectl get nodes**
NAME          STATUS   ROLES           AGE     VERSION
k8s-master    Ready    control-plane   3d22h   v1.24.1
k8s-worker1   Ready    <none>          3d22h   v1.24.1
k8s-worker2   Ready    <none>          3d22h   v1.24.1
```

## 03. Drain

drainì€ ë°°ìˆ˜í•˜ë‹¤ë€ ëœ»ì´ë‹¤. í•˜ìˆ˜êµ¬ì— ë¬¼ ë¹ ì§€ë“¯ì´ íŠ¹ì • ë…¸ë“œì—ì„œ ì‹¤í–‰ì¤‘ì¸ ëª¨ë“  podë¥¼ ë‹¤ë¥¸ nodeë¡œ ì˜®ê¸°ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.

**ë‹¨,** **deploymentë‚˜ daemonset ë“±ìœ¼ë¡œ ê´€ë¦¬ë˜ì§€ ì•ŠëŠ” ë…ë¦½ì ì¸ Podë“¤ì€ ê·¸ëŒ€ë¡œ ì‚­ì œë˜ë‹ˆ ì£¼ì˜í•´ì•¼ í•œë‹¤.**

### 01. Drain ì‚¬ìš©

<aside>
ğŸ’¡ **# kubectl drain {ë…¸ë“œëª…}**

</aside>

ìœ„ì˜ ëª…ë ¹ì–´ëŠ” ê¸°ë³¸í˜•ì´ë‹¤. ë§Œì•½ drainì„ ì‹œë„í•˜ëŠ” ë…¸ë“œì—ì„œ ê°œë³„ podë‚˜ daemonset, emptydirì„ ì‚¬ìš©í•˜ëŠ” Podê°€ ìˆë‹¤ë©´ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ì„œ drainì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.

```json
**# kubectl drain k8s-worker1**
node/k8s-worker1 cordoned
error: unable to drain node "k8s-worker1" due to error:[cannot delete Pods with local storage (use --delete-emptydir-data to override): default/emptdir-sidecar-pod, cannot delete Pods declare no controller (use --force to override): echo/nginx, echo/nginx-80, my-app/curly, my-app/test-ingress, my-app/wget, cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-sqbl2, kube-system/kube-proxy-fzqph], continuing command...
There are pending nodes to be drained:
 k8s-worker1
**cannot delete Pods with local storage (use --delete-emptydir-data to override): default/emptdir-sidecar-pod   <- emptDir ì‚¬ìš©ë•Œë¬¸ì— ì‚­ì œ ë¶ˆê°€**
**cannot delete Pods declare no controller (use --force to override): echo/nginx, echo/nginx-80, my-app/curly, my-app/test-ingress, my-app/wget <- ë‹¨ì¼ Pod ë•Œë¬¸ì— ì‚­ì œ ë¶ˆê°€**
**cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-sqbl2, kube-system/kube-proxy-fzqph <- DaemonSet ë•Œë¬¸ì— ì‚­ì œ ë¶ˆê°€**
```

ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ì¼ë¶€ ì˜µì…˜ì„ ì¶”ê°€í•´ì„œ ì§„í–‰í•´ì•¼í•œë‹¤.

<aside>
ğŸ’¡ **# kubectl drain {ë…¸ë“œëª…} --ignore-daemonsets --delete-emptydir-data --force**

</aside>

- --ignore-daemonsets : ë°ëª¬ì…‹ìœ¼ë¡œ ì‹¤í–‰ëœ Podë¥¼ ë¬´ì‹œí•œë‹¤.
- --delete-emptydir-data : emptyDirë¡œ ì„ ì–¸ëœ ë³¼ë¥¨ì˜ ë°ì´í„°ë¥¼ ì‚­ì œí•œë‹¤.
- --force : ë‹¨ì¼ Podë¥¼ ì‚­ì œí•œë‹¤.

```json
**# kubectl drain k8s-worker1 --ignore-daemonsets --delete-emptydir-data --force** 
**node/k8s-worker1 already cordoned <- í•´ë‹¹ ë…¸ë“œë¥¼ cordon ì²˜ë¦¬**
**WARNING: deleting Pods that declare no controller: default/emptdir-sidecar-pod, echo/nginx, echo/nginx-80, my-app/curly, my-app/test-ingress, my-app/wget; ignoring DaemonSet-managed Pods: kube-system/calico-node-sqbl2, kube-system/kube-proxy-fzqph <- ë‹¨ì¼ PodëŠ” ì‚­ì œë˜ì§€ë§Œ DaemonSet PodëŠ” ë¬´ì‹œëœë‹¤ëŠ” ë©”ì‹œì§€**
evicting pod my-app/wget
evicting pod default/emptdir-sidecar-pod
evicting pod default/taint-toleration-test-688dd8b89-m9xlw
evicting pod default/taint-toleration-test-688dd8b89-xf5lx
evicting pod echo/nginx-80
evicting pod echo/nginx
evicting pod my-app/curly
evicting pod my-app/test-ingress
evicting pod default/taint-toleration-test-688dd8b89-b6kcg
pod/curly evicted
pod/taint-toleration-test-688dd8b89-xf5lx evicted
pod/emptdir-sidecar-pod evicted
pod/taint-toleration-test-688dd8b89-m9xlw evicted
pod/nginx-80 evicted
pod/taint-toleration-test-688dd8b89-b6kcg evicted
pod/nginx evicted
pod/test-ingress evicted
pod/wget evicted
node/k8s-worker1 drained

**# kubectl describe node k8s-worker1
(ì „ëµ)
## DaemonSetì´ ê´€ë¦¬í•˜ëŠ” Podë§Œ ì‹¤í–‰ì¤‘**
Non-terminated Pods:          (2 in total)
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                 ------------  ----------  ---------------  -------------  ---
  **kube-system                 calico-node-sqbl2**    250m (6%)     0 (0%)      0 (0%)           0 (0%)         3d23h
  **kube-system                 kube-proxy-fzqph**     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d23h
```
## 03. Drain

drainì€ ë°°ìˆ˜í•˜ë‹¤ë€ ëœ»ì´ë‹¤. í•˜ìˆ˜êµ¬ì— ë¬¼ ë¹ ì§€ë“¯ì´ íŠ¹ì • ë…¸ë“œì—ì„œ ì‹¤í–‰ì¤‘ì¸ ëª¨ë“  podë¥¼ ë‹¤ë¥¸ nodeë¡œ ì˜®ê¸°ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.

**ë‹¨,** **deploymentë‚˜ daemonset ë“±ìœ¼ë¡œ ê´€ë¦¬ë˜ì§€ ì•ŠëŠ” ë…ë¦½ì ì¸ Podë“¤ì€ ê·¸ëŒ€ë¡œ ì‚­ì œë˜ë‹ˆ ì£¼ì˜í•´ì•¼ í•œë‹¤.**

### 01. Drain ì‚¬ìš©

<aside>
ğŸ’¡ **# kubectl drain {ë…¸ë“œëª…}**

</aside>

ìœ„ì˜ ëª…ë ¹ì–´ëŠ” ê¸°ë³¸í˜•ì´ë‹¤. ë§Œì•½ drainì„ ì‹œë„í•˜ëŠ” ë…¸ë“œì—ì„œ ê°œë³„ podë‚˜ daemonset, emptydirì„ ì‚¬ìš©í•˜ëŠ” Podê°€ ìˆë‹¤ë©´ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ì„œ drainì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.

```json
**# kubectl drain k8s-worker1**
node/k8s-worker1 cordoned
error: unable to drain node "k8s-worker1" due to error:[cannot delete Pods with local storage (use --delete-emptydir-data to override): default/emptdir-sidecar-pod, cannot delete Pods declare no controller (use --force to override): echo/nginx, echo/nginx-80, my-app/curly, my-app/test-ingress, my-app/wget, cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-sqbl2, kube-system/kube-proxy-fzqph], continuing command...
There are pending nodes to be drained:
 k8s-worker1
**cannot delete Pods with local storage (use --delete-emptydir-data to override): default/emptdir-sidecar-pod   <- emptDir ì‚¬ìš©ë•Œë¬¸ì— ì‚­ì œ ë¶ˆê°€**
**cannot delete Pods declare no controller (use --force to override): echo/nginx, echo/nginx-80, my-app/curly, my-app/test-ingress, my-app/wget <- ë‹¨ì¼ Pod ë•Œë¬¸ì— ì‚­ì œ ë¶ˆê°€**
**cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-sqbl2, kube-system/kube-proxy-fzqph <- DaemonSet ë•Œë¬¸ì— ì‚­ì œ ë¶ˆê°€**
```

ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ì¼ë¶€ ì˜µì…˜ì„ ì¶”ê°€í•´ì„œ ì§„í–‰í•´ì•¼í•œë‹¤.

<aside>
ğŸ’¡ **# kubectl drain {ë…¸ë“œëª…} --ignore-daemonsets --delete-emptydir-data --force**

</aside>

- --ignore-daemonsets : ë°ëª¬ì…‹ìœ¼ë¡œ ì‹¤í–‰ëœ Podë¥¼ ë¬´ì‹œí•œë‹¤.
- --delete-emptydir-data : emptyDirë¡œ ì„ ì–¸ëœ ë³¼ë¥¨ì˜ ë°ì´í„°ë¥¼ ì‚­ì œí•œë‹¤.
- --force : ë‹¨ì¼ Podë¥¼ ì‚­ì œí•œë‹¤.

```json
**# kubectl drain k8s-worker1 --ignore-daemonsets --delete-emptydir-data --force** 
**node/k8s-worker1 already cordoned <- í•´ë‹¹ ë…¸ë“œë¥¼ cordon ì²˜ë¦¬**
**WARNING: deleting Pods that declare no controller: default/emptdir-sidecar-pod, echo/nginx, echo/nginx-80, my-app/curly, my-app/test-ingress, my-app/wget; ignoring DaemonSet-managed Pods: kube-system/calico-node-sqbl2, kube-system/kube-proxy-fzqph <- ë‹¨ì¼ PodëŠ” ì‚­ì œë˜ì§€ë§Œ DaemonSet PodëŠ” ë¬´ì‹œëœë‹¤ëŠ” ë©”ì‹œì§€**
evicting pod my-app/wget
evicting pod default/emptdir-sidecar-pod
evicting pod default/taint-toleration-test-688dd8b89-m9xlw
evicting pod default/taint-toleration-test-688dd8b89-xf5lx
evicting pod echo/nginx-80
evicting pod echo/nginx
evicting pod my-app/curly
evicting pod my-app/test-ingress
evicting pod default/taint-toleration-test-688dd8b89-b6kcg
pod/curly evicted
pod/taint-toleration-test-688dd8b89-xf5lx evicted
pod/emptdir-sidecar-pod evicted
pod/taint-toleration-test-688dd8b89-m9xlw evicted
pod/nginx-80 evicted
pod/taint-toleration-test-688dd8b89-b6kcg evicted
pod/nginx evicted
pod/test-ingress evicted
pod/wget evicted
node/k8s-worker1 drained

**# kubectl describe node k8s-worker1
(ì „ëµ)
## DaemonSetì´ ê´€ë¦¬í•˜ëŠ” Podë§Œ ì‹¤í–‰ì¤‘**
Non-terminated Pods:          (2 in total)
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                 ------------  ----------  ---------------  -------------  ---
  **kube-system                 calico-node-sqbl2**    250m (6%)     0 (0%)      0 (0%)           0 (0%)         3d23h
  **kube-system                 kube-proxy-fzqph**     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d23h
```

### 02. Drain ì¢…ë£Œ

Drainì€ ìš°ì„  Nodeë¥¼ Cordon ì„¤ì •í•œ ë’¤ ëª¨ë“  Podë“¤ì„ ì˜®ê¸°ëŠ” ëª…ë ¹ì–´ë‹¤. Drainì„ ì¢…ë£Œí•œë‹¤ëŠ” ëœ»ì€ ê²°êµ­ uncordon ì„¤ì •í•œë‹¤ëŠ” ëœ»ê³¼ ê°™ë‹¤.

<aside>
ğŸ’¡ **# kubectl uncordon {ë…¸ë“œëª…}**

</aside>

```json
# **kubectl get nodes**
NAME          STATUS                     ROLES           AGE     VERSION
k8s-master    Ready                      control-plane   3d22h   v1.24.1
k8s-worker1   Ready**,SchedulingDisabled**   <none>          3d22h   v1.24.1
k8s-worker2   Ready    

**# kubectl uncordon k8s-worker1**
node/k8s-worker1 uncordoned

# **kubectl get nodes**
NAME          STATUS   ROLES           AGE     VERSION
k8s-master    Ready    control-plane   3d22h   v1.24.1
k8s-worker1   Ready    <none>          3d22h   v1.24.1
k8s-worker2   Ready    <none>          3d22h   v1.24.1
```
